{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9968811f",
   "metadata": {},
   "source": [
    "# Fruty — Experiment Notebook\n",
    "\n",
    "Goal: reproduce and iterate the IDS experiments (NSL-KDD) end-to-end and push detection performance as high as possible.\n",
    "\n",
    "Notes: we distinguish two goals — (a) binary detection (normal vs attack), where ~99% accuracy is realistic, and (b) multiclass attack classification (many imbalanced classes) which is harder. This notebook contains diagnostics, fast filter + ABA feature selection, weighted LightGBM and CatBoost baselines, and stacking/ensemble scaffolding.\n",
    "\n",
    "FAST_RUN: toggle to `True` to run quick/smaller experiments (useful for CI / local iteration). Set to `False` for full runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "FAST_RUN = True\n",
    "SEED = 1\n",
    "import os, sys, json, time\n",
    "from pathlib import Path\n",
    "ROOT = Path('..').resolve().parent if Path('.').name == 'notebooks' else Path('.').resolve()\n",
    "# adjust sys.path to import project modules\n",
    "sys.path.insert(0, str(ROOT))\n",
    "print('Root:', ROOT)\n",
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper to load processed arrays\n",
    "from pathlib import Path\n",
    "def load_processed():\n",
    "    tr = Path('data') / 'processed' / 'train_processed.npz'\n",
    "    te = Path('data') / 'processed' / 'test_processed.npz'\n",
    "    if not tr.exists() or not te.exists():\n",
    "        raise FileNotFoundError('Run src/preprocessing.py to create data/processed/*.npz')\n",
    "    dt = np.load(tr, allow_pickle=True)\n",
    "    de = np.load(te, allow_pickle=True)\n",
    "    return dt['X'], dt['y'], de['X'], de['y']\n",
    "# quick smoke load\n",
    "Xtr, ytr, Xte, yte = load_processed()\n",
    "print('Loaded processed arrays: ', Xtr.shape, Xte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b442d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Diagnostics (per-feature variance, NaNs, label counts)\n",
    "def diagnostics(X, y, top_n=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    var = np.nanvar(X, axis=0)\n",
    "    n_zero = np.sum(X == 0, axis=0)\n",
    "    n_nan = np.sum(np.isnan(X), axis=0)\n",
    "    df = pd.DataFrame({\n",
    "        'feature_idx': np.arange(n_features),\n",
    "        'variance': var,\n",
    "        'pct_zero': n_zero / n_samples,\n",
    "        'pct_nan': n_nan / n_samples,\n",
    "    })\n",
    "    df = df.sort_values('variance', ascending=False).reset_index(drop=True)\n",
    "    print('n_features:', n_features)\n",
    "    print('constant features (var==0):', int((df['variance'] == 0).sum()))\n",
    "    print('top features by variance:')\n",
    "    display(df.head(top_n))\n",
    "    print('\n",
    "Label distribution (train):')\n",
    "    from collections import Counter\n",
    "    print(Counter(y.tolist()))\n",
    "    out = Path('results')\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out / 'feature_diagnostics.csv', index=False)\n",
    "    print('Saved feature_diagnostics.csv')\n",
    "\n",
    "diagnostics(Xtr, ytr, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Pre-filter using f_classif (fast) and save top-K indices\n",
    "from sklearn.feature_selection import f_classif\n",
    "K = 500\n",
    "mi_subsample = 5000 if not FAST_RUN else 2000\n",
    "print('Using subsample for f_classif:', mi_subsample)\n",
    "sss_idx = None\n",
    "if Xtr.shape[0] > mi_subsample:\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=mi_subsample, random_state=SEED)\n",
    "    idx, _ = next(sss.split(Xtr, ytr))\n",
    "    Xs = Xtr[idx]\n",
    "    ys = ytr[idx]\n",
    "else:\n",
    "    Xs = Xtr; ys = ytr\n",
    "f_vals, p_vals = f_classif(Xs, ys)\n",
    "top_k = int(K if K <= Xtr.shape[1] else Xtr.shape[1])\n",
    "top_idx = np.argsort(f_vals)[-top_k:][::-1]\n",
    "print('Selected top_k=', top_k)\n",
    "np.save('results/top_k_indices.npy', top_idx)\n",
    "print('Saved results/top_k_indices.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run ABA on reduced feature set (fast mode).\n",
    "# This cell will import the small ABA implementation in src/feature_selection/aba.py\n",
    "from src.feature_selection.aba import ArtificialButterfly\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "# map reduced space -> original indices\n",
    "top_idx = np.load('results/top_k_indices.npy')\n",
    "Xtr_red = Xtr[:, top_idx]\n",
    "Xte_red = Xte[:, top_idx]\n",
    "print('Reduced shapes:', Xtr_red.shape, Xte_red.shape)\n",
    "# define fitness: class-weighted LGB with 3-fold CV scoring f1_macro\n",
    "def make_lgb_fitness(X, y, n_splits=3, seed=SEED):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    clf = lgb.LGBMClassifier(n_estimators=50 if not FAST_RUN else 10, learning_rate=0.1, random_state=seed, n_jobs=1)\n",
    "    def fitness(X_sub, y_sub):\n",
    "        if X_sub.shape[1] == 0:\n",
    "            return 0.0\n",
    "        try:\n",
    "            scores = cross_val_score(clf, X_sub, y_sub, cv=skf, scoring='f1_macro', n_jobs=1)\n",
    "            return float(np.mean(scores))\n",
    "        except Exception as e:\n",
    "            print('Fitness eval failed:', e)\n",
    "            return 0.0\n",
    "    return fitness\n",
    "\n",
    "fitness = make_lgb_fitness(Xtr_red, ytr, n_splits=3)\n",
    "aba_pop = 12 if not FAST_RUN else 6\n",
    "aba_iter = 20 if not FAST_RUN else 6\n",
    "print('ABA settings pop,iter =', aba_pop, aba_iter)\n",
    "aba = ArtificialButterfly(pop_size=aba_pop, n_iter=aba_iter, random_state=SEED)\n",
    "# run ABA (this may take time)\n",
    "best_mask_red, best_score = aba.fit(Xtr_red, ytr, fitness)\n",
    "print('ABA done: best_score=', best_score, 'n_features_selected=', int(best_mask_red.sum()))\n",
    "# map back to original feature indices and save\n",
    "sel_indices = top_idx[best_mask_red.astype(bool)]\n",
    "np.save('models/aba_best_mask.npy', sel_indices)\n",
    "# save history\n",
    "import csv\n",
    "with open('results/aba_history.csv', 'w', newline='') as fh:\n",
    "    w = csv.writer(fh)\n",
    "    w.writerow(['iteration','best_score'])\n",
    "    for i,s in enumerate(aba.history_):\n",
    "        w.writerow([i,s])\n",
    "print('Saved models/aba_best_mask.npy and results/aba_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Retrain final model using ABA-selected features (or fallback to top_k) and evaluate on test set\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "# load selected features if present\n",
    "sel_path = Path('models') / 'aba_best_mask.npy'\n",
    "if sel_path.exists():\n",
    "    sel = np.load(sel_path)\n",
    "    sel_mask = np.zeros(Xtr.shape[1], dtype=bool)\n",
    "    sel_mask[sel] = True\n",
    "    print('Using ABA-selected features count=', sel_mask.sum())\n",
    "else:\n",
    "    print('ABA mask not found: using top_k indices')\n",
    "    sel = np.load('results/top_k_indices.npy')\n",
    "    sel_mask = np.zeros(Xtr.shape[1], dtype=bool)\n",
    "    sel_mask[sel] = True\n",
    "    print('Using top_k features count=', sel_mask.sum())\n",
    "Xtr_sel = Xtr[:, sel_mask]\n",
    "Xte_sel = Xte[:, sel_mask]\n",
    "# train a final weighted LGB on a stratified subsample for speed, or full if FAST_RUN False\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "tr_size = 30000 if not FAST_RUN else 8000\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=min(tr_size, Xtr_sel.shape[0]), random_state=SEED)\n",
    "idx_train, _ = next(sss.split(Xtr_sel, ytr))\n",
    "Xtrain_sub = Xtr_sel[idx_train]\n",
    "ytrain_sub = ytr[idx_train]\n",
    "# compute sample weights inverse freq (simple)\n",
    "from collections import Counter\n",
    "counts = Counter(ytrain_sub.tolist())\n",
    "total = len(ytrain_sub)\n",
    "class_weight = {k: total / (len(counts) * v) for k, v in counts.items()}\n",
    "sample_weight = np.array([class_weight[int(l)] for l in ytrain_sub])\n",
    "clf = lgb.LGBMClassifier(n_estimators=200 if not FAST_RUN else 50, learning_rate=0.05, random_state=SEED, n_jobs=1)\n",
    "clf.fit(Xtrain_sub, ytrain_sub, sample_weight=sample_weight)\n",
    "y_pred = clf.predict(Xte_sel)\n",
    "acc = accuracy_score(yte, y_pred)\n",
    "f1 = f1_score(yte, y_pred, average='macro')\n",
    "print('Final model -> acc:', acc, 'f1_macro:', f1)\n",
    "# save final model and results\n",
    "joblib.dump(clf, 'models/aba_lgb_final.joblib')\n",
    "pd.DataFrame([{'setup':'aba_lgb_final','accuracy':acc,'f1_macro':f1,'n_features':int(sel_mask.sum())}]).to_csv('results/aba_results.csv', index=False)\n",
    "print('Saved models/aba_lgb_final.joblib and results/aba_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba23feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Binary detection validation (normal vs attack)\n",
    "def to_binary(y):\n",
    "    ys = np.array([str(v).lower() for v in y])\n",
    "    return (ys != 'normal').astype(int)\n",
    "ytr_bin = to_binary(ytr)\n",
    "yte_bin = to_binary(yte)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "clf_bin = lgb.LGBMClassifier(n_estimators=100 if not FAST_RUN else 30, random_state=SEED, n_jobs=1)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "# evaluate with CV on subsample to avoid long runs\n",
    "scores = cross_val_score(clf_bin, Xtr, ytr_bin, cv=skf, scoring='f1', n_jobs=1)\n",
    "print('Binary cv f1 (5-fold) mean/std:', scores.mean(), scores.std())\n",
    "clf_bin.fit(Xtr, ytr_bin)\n",
    "y_pred_bin = clf_bin.predict(Xte)\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "print('Binary test acc:', accuracy_score(yte_bin, y_pred_bin))\n",
    "print('Binary test f1:', f1_score(yte_bin, y_pred_bin))\n",
    "# save binary model\n",
    "joblib.dump(clf_bin, 'models/lgb_binary_full.joblib')\n",
    "print('Saved models/lgb_binary_full.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb521d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Optional CatBoost experiment (handles categorical natively)\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    cat_available = True\n",
    "except Exception:\n",
    "    cat_available = False\n",
    "\n",
    "if cat_available:\n",
    "    print('CatBoost available — running a quick experiment (fast mode subsample)')\n",
    "    # To run CatBoost using original categorical columns we would need the original DataFrame and pipeline.\n",
    "    # For speed, we run CatBoost on numeric processed features as a comparison.\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=8000 if FAST_RUN else 30000, random_state=SEED)\n",
    "    idx, _ = next(sss.split(Xtr, ytr))\n",
    "    Xs = Xtr[idx]; ys = ytr[idx]\n",
    "    clf_cat = CatBoostClassifier(iterations=200 if not FAST_RUN else 50, random_seed=SEED, verbose=False)\n",
    "    clf_cat.fit(Xs, ys)\n",
    "    ycat = clf_cat.predict(Xte)\n",
    "    print('CatBoost multiclass f1_macro:', f1_score(yte, ycat, average='macro'))\n",
    "    joblib.dump(clf_cat, 'models/catboost_quick.joblib')\n",
    "else:\n",
    "    print('CatBoost not installed — skip this cell or install catboost in the venv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c53d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Simple stacking ensemble scaffold (LGB + optional CatBoost + LR meta)\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "estimators = [('lgb', clf)]\n",
    "if 'clf_cat' in globals() and cat_available:\n",
    "    estimators.append(('cat', clf_cat))\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=200), n_jobs=1)\n",
    "# quick CV to evaluate stacking (fast)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "sc = cross_val_score(stack, Xtr[:8000] if FAST_RUN else Xtr, ytr[:8000] if FAST_RUN else ytr, cv=3, scoring='f1_macro', n_jobs=1)\n",
    "print('Stacking cv f1_macro (3-fold):', sc.mean(), sc.std())\n",
    "# Save placeholder: full fitting left as next step if stacking improves CV\n",
    "print('If stacking helps, fit on full training set and evaluate on test set (left as follow-up).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6499b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save experiment config and short conclusions\n",
    "cfg = {\n",
    "    'date': time.asctime(),\n",
    "    'seed': SEED,\n",
    "    'FAST_RUN': FAST_RUN,\n",
    "    'aba_pop': aba_pop if 'aba_pop' in globals() else None,\n",
    "    'aba_iter': aba_iter if 'aba_iter' in globals() else None,\n",
    "    'top_k': int(top_k) if 'top_k' in globals() else None\n",
    "}\n",
    "Path('results').mkdir(parents=True, exist_ok=True)\n",
    "with open('results/experiment_config.json', 'w') as fh:\n",
    "    json.dump(cfg, fh, indent=2)\n",
    "print('Saved results/experiment_config.json')\n",
    "\n",
    "print('Next steps (short):')\n",
    "print('- Validate binary detector via repeated CV and different splits (aim for stable 99%+).')\n",
    "print('- Run ABA full-scale on top-1000 features (longer run) and hybridize with PSO/GA.')\n",
    "print('- Train CatBoost using original categorical pipeline (avoids OHE explosion) and compare.')\n",
    "print('- Implement adaptive voting based on per-sample confidences for stacking/ensemble.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
